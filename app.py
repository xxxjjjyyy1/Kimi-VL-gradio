import gradio as gr
from PIL import Image
from transformers import AutoModelForCausalLM, AutoProcessor
import os
import time
from datetime import datetime
import re

# åˆå§‹åŒ–å…¨å±€æ¨¡å¼
global_mode = "Compact mode"

# åŠ è½½å¤šæ¨¡æ€æ¨¡å‹
model_path = "/openbayes/input/input0/Kimi-VL-A3B-Instruct"
model_path_thinking = "/openbayes/input/input0/Kimi-VL-A3B-Thinking"
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype="auto",
    device_map="auto",
    trust_remote_code=True,
    local_files_only=True
)
model_thinking = AutoModelForCausalLM.from_pretrained(
    model_path_thinking,
    torch_dtype="auto",
    device_map="auto",
    trust_remote_code=True,
    local_files_only=True
)

processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True, local_files_only=True)
processor_thinking = AutoProcessor.from_pretrained(model_path_thinking, trust_remote_code=True, local_files_only=True)

# é…ç½®å›¾ç‰‡å­˜å‚¨è·¯å¾„
save_image_path = "./uploaded_images/"
os.makedirs(save_image_path, exist_ok=True)


def animate_loading(dots):
    """ç”ŸæˆåŠ¨æ€çš„...æ•ˆæœ"""
    return "..." + "." * (dots % 4)


def process_input(text, image_path, mode, history):
    global global_mode
    global_mode = mode
    current_history = history.copy()  # å…³é”®ä¿®æ”¹ï¼šå¤åˆ¶å†å²è®°å½•
    if global_mode == "Detailed mode":
        # å‡†å¤‡æ˜¾ç¤ºå†…å®¹
        user_input_display = text or "(æ— æ–‡å­—)"
        if image_path:
            user_input_display += " ğŸ“·"
            try:
                image = Image.open(image_path)
            except Exception as e:
                error_msg = f"å›¾ç‰‡åŠ è½½å¤±è´¥: {str(e)}"
                updated_history = current_history + [(user_input_display, error_msg)]
                yield updated_history, updated_history
                return
        else:
            image = None

        # ç¬¬ä¸€é˜¶æ®µï¼šæ˜¾ç¤ºåˆå§‹åŠ è½½çŠ¶æ€
        updated_history = current_history + [(user_input_display, animate_loading(0))]
        yield updated_history, updated_history

        # ç¬¬äºŒé˜¶æ®µï¼šåå°å¤„ç†ä¸åŠ è½½åŠ¨ç”»
        response = None
        error = None
        try:
            # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯
            messages = []
            if text:
                messages.append({"role": "user", "content": [{"type": "text", "text": text}]})
            if image:
                messages.append({"role": "user", "content": [{"type": "image", "image": image_path}]})

            # å‡†å¤‡æ¨¡å‹è¾“å…¥
            text_prompt = processor_thinking.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt")

            if image and text:
                inputs = processor_thinking(
                    images=image,
                    text=text_prompt,
                    return_tensors="pt",
                    padding=True,
                    truncation=True
                ).to(model_thinking.device)
            elif text:
                inputs = processor_thinking(
                    text=text_prompt,
                    return_tensors="pt",
                    padding=True,
                    truncation=True
                ).to(model_thinking.device)
            elif image:
                inputs = processor_thinking(
                    images=image,
                    return_tensors="pt",
                    padding=True,
                    truncation=True
                ).to(model_thinking.device)

            # ç”Ÿæˆå“åº”
            generated_ids = model_thinking.generate(**inputs, max_new_tokens=512)
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            response = processor_thinking.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )[0]


        except Exception as e:
            error = f"å¤„ç†é”™è¯¯: {str(e)}"

        # æ›´æ–°æœ€ç»ˆç»“æœ
        final_response = response if response else error
        if "â—thinkâ–·" in final_response:
            final_response = re.sub(
                r'â—thinkâ–·(.*?)â—/thinkâ–·',
                lambda m: f"ğŸ¤” æ€è€ƒè¿‡ç¨‹ï¼š\n{m.group(1)}\n\nğŸ“ æ­£å¼å›ç­”ï¼š\n",
                final_response,
                flags=re.DOTALL
            )
        final_history = current_history + [(user_input_display, final_response)]
        yield final_history, final_history
    else:
        # å‡†å¤‡æ˜¾ç¤ºå†…å®¹
        user_input_display = text or "(æ— æ–‡å­—)"
        if image_path:
            user_input_display += " ğŸ“·"
            try:
                image = Image.open(image_path)
            except Exception as e:
                error_msg = f"å›¾ç‰‡åŠ è½½å¤±è´¥: {str(e)}"
                updated_history = history + [(user_input_display, error_msg)]
                yield updated_history, updated_history
                return
        else:
            image = None

        # ç¬¬ä¸€é˜¶æ®µï¼šæ˜¾ç¤ºåˆå§‹åŠ è½½çŠ¶æ€
        updated_history = current_history + [(user_input_display, animate_loading(0))]
        yield updated_history, updated_history

        # ç¬¬äºŒé˜¶æ®µï¼šåå°å¤„ç†ä¸åŠ è½½åŠ¨ç”»
        response = None
        error = None
        try:
            # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯
            messages = []
            if text:
                messages.append({"role": "user", "content": [{"type": "text", "text": text}]})
            if image:
                messages.append({"role": "user", "content": [{"type": "image", "image": image_path}]})

            # å‡†å¤‡æ¨¡å‹è¾“å…¥
            text_prompt = processor.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt")

            if image and text:
                inputs = processor(
                    images=image,
                    text=text_prompt,
                    return_tensors="pt",
                    padding=True,
                    truncation=True
                ).to(model.device)
            elif text:
                inputs = processor(
                    text=text_prompt,
                    return_tensors="pt",
                    padding=True,
                    truncation=True
                ).to(model.device)
            elif image:
                inputs = processor(
                    images=image,
                    return_tensors="pt",
                    padding=True,
                    truncation=True
                ).to(model.device)

            # ç”Ÿæˆå“åº”
            generated_ids = model.generate(**inputs, max_new_tokens=512)
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            response = processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )[0]

        except Exception as e:
            error = f"å¤„ç†é”™è¯¯: {str(e)}"

        # æ›´æ–°æœ€ç»ˆç»“æœ
        final_response = response if response else error
        final_history = current_history + [(user_input_display, final_response)]
        yield final_history, final_history


def capture_and_process(text_input, image_input, mode, history):
    current_history = history.copy()  # å…³é”®ä¿®æ”¹ï¼šä¿æŒå†å²è¿ç»­æ€§
    """å¤„ç†è¾“å…¥å¹¶ä¿æŒåŠ è½½åŠ¨ç”»"""
    # æ¸…ç©ºè¾“å…¥å¹¶ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
    image_path = None
    if image_input:
        try:
            image_path = os.path.join(save_image_path, f"temp_{int(time.time())}.png")
            image_input.save(image_path)
        except Exception as e:
            error_msg = f"å›¾ç‰‡ä¿å­˜å¤±è´¥: {str(e)}"
            return history + [(error_msg, "")], history + [(error_msg, "")], "", None

    # è¿”å›æ¸…ç©ºåçš„è¾“å…¥ç»„ä»¶å€¼
    return "", None, text_input, image_path, current_history


with gr.Blocks(title="Kimi-VL") as demo:
    gr.Markdown("## Kimi-VL")

    with gr.Row():
        chatbot = gr.Chatbot(label="Session flow", height=500, elem_classes=["dots"])
        history = gr.State([])

    with gr.Row():
        with gr.Column(scale=3):
            text_input = gr.Textbox(label="Input text", placeholder="Enter text content...", lines=4)
            image_input = gr.Image(label="Upload pictures", type="pil", height=200)

        with gr.Column(scale=1):
            mode = gr.Dropdown(choices=["Compact mode", "Detailed mode"], value="Compact mode", label="Response mode")
            submit_btn = gr.Button("send", variant="primary")
            clear_btn = gr.Button("Clear history", variant="secondary")

    temp_text = gr.State()
    temp_image = gr.State()

    submit_btn.click(
        capture_and_process,
        inputs=[text_input, image_input, mode, history],
        outputs=[text_input, image_input, temp_text, temp_image, history],
        queue=False
    ).then(
        process_input,
        inputs=[temp_text, temp_image, mode, history],
        outputs=[chatbot, history]
    )

    clear_btn.click(
        lambda: ([], []),
        inputs=[],
        outputs=[chatbot, history]
    )

demo.launch(server_name="0.0.0.0", server_port=8080)